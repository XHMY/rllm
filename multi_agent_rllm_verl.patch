diff --git a/verl/experimental/agent_loop/agent_loop.py b/verl/experimental/agent_loop/agent_loop.py
index ef863810..9f465097 100644
--- a/verl/experimental/agent_loop/agent_loop.py
+++ b/verl/experimental/agent_loop/agent_loop.py
@@ -28,6 +28,7 @@ from omegaconf import DictConfig, OmegaConf
 from pydantic import BaseModel
 from tensordict import TensorDict
 from transformers import AutoTokenizer
+from vllm.lora.request import LoRARequest
 
 from verl.protocol import DataProto
 from verl.single_controller.ray.base import RayWorkerGroup
@@ -84,6 +85,7 @@ class AsyncLLMServerManager:
         *,
         prompt_ids: list[int],
         sampling_params: dict[str, Any],
+        lora_request: LoRARequest
     ) -> list[int]:
         """Generate tokens from prompt ids.
 
@@ -100,6 +102,7 @@ class AsyncLLMServerManager:
             request_id=request_id,
             prompt_ids=prompt_ids,
             sampling_params=sampling_params,
+            lora_request=lora_request,
         )
         return output
 
@@ -534,6 +537,20 @@ class AgentLoopManager:
 
         return timing
 
+    def load_lora_adapter(self, lora_name: str, lora_path: str):
+        """Load LoRA adapter to all rollout server instances.
+
+        Args:
+            lora_name (str): LoRA adapter name.
+            lora_path (str): LoRA adapter path.
+        """
+        ray.get(
+            [
+                server.load_lora_adapter.remote(lora_name=lora_name, lora_path=lora_path)
+                for server in self.async_llm_servers
+            ]
+        )
+
     def wake_up(self):
         """Wake up all rollout server instances."""
         ray.get([server.wake_up.remote() for server in self.async_llm_servers])
diff --git a/verl/utils/dataset/rl_dataset.py b/verl/utils/dataset/rl_dataset.py
index fcbdd38b..2b37e563 100644
--- a/verl/utils/dataset/rl_dataset.py
+++ b/verl/utils/dataset/rl_dataset.py
@@ -15,6 +15,7 @@
 # limitations under the License.
 
 import copy
+import json
 import logging
 import os
 import re
@@ -315,6 +316,9 @@ class RLHFDataset(Dataset):
         if self.return_full_prompt:
             row_dict["full_prompts"] = raw_prompt  # array of strings
 
+        extra_info = row_dict.get("extra_info", {})
+        if type(extra_info) is str:
+            row_dict["extra_info"] = json.loads(extra_info)
         # add index for each prompt
         index = row_dict.get("extra_info", {}).get("index", 0)
         tools_kwargs = row_dict.get("extra_info", {}).get("tools_kwargs", {})
diff --git a/verl/utils/fsdp_utils.py b/verl/utils/fsdp_utils.py
index de1bea4b..411c29e9 100644
--- a/verl/utils/fsdp_utils.py
+++ b/verl/utils/fsdp_utils.py
@@ -522,7 +522,7 @@ def fsdp2_clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinit
     return total_norm
 
 
-def layered_summon_lora_params(fsdp_module) -> OrderedDict:
+def layered_summon_lora_params(fsdp_module, adapter_name="default") -> OrderedDict:
     from peft.utils.save_and_load import get_peft_model_state_dict
 
     def __prefix_submodules(module, prefix):
@@ -549,7 +549,7 @@ def layered_summon_lora_params(fsdp_module) -> OrderedDict:
                 continue
             if fsdp_version(submodule) > 0:
                 with FSDP.summon_full_params(submodule, writeback=False):
-                    sub_lora_params = get_peft_model_state_dict(peft_model, state_dict=submodule.state_dict())
+                    sub_lora_params = get_peft_model_state_dict(peft_model, state_dict=submodule.state_dict(), adapter_name=adapter_name)
                     sub_lora_params = {
                         f"{prefix}.{name}": param.full_tensor().detach().cpu()
                         if hasattr(param, "full_tensor")
diff --git a/verl/workers/fsdp_workers.py b/verl/workers/fsdp_workers.py
index 4141d986..bde18e6c 100644
--- a/verl/workers/fsdp_workers.py
+++ b/verl/workers/fsdp_workers.py
@@ -326,6 +326,28 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
                     "bias": "none",
                 }
                 actor_module = get_peft_model(actor_module, LoraConfig(**lora_config))
+
+                # Check if multi-agent mode is enabled
+                multi_agent_names = self.config.get("agent_names", [])
+                if len(multi_agent_names) > 1 and (not self.config.get("ori_single_policy_no_lora_mode", False)):
+                    # Multi-agent: Initialize all LoRA adapters before FSDP wrapping
+                    print(f"Multi-agent LoRA mode: {len(multi_agent_names)} adapters")
+
+                    for adapter_name in multi_agent_names:
+                        print(f"Adding adapter: {adapter_name}")
+                        actor_module.add_adapter(adapter_name=adapter_name, peft_config=LoraConfig(**lora_config))
+
+                    # FSDP auto wrapping requires all parameters to have requires_grad set correctly
+                    for name, param in actor_module.named_parameters():
+                        if "lora" in name and "default" not in name:
+                            param.requires_grad = True
+                        else:
+                            param.requires_grad = False
+
+                    self.current_active_lora = multi_agent_names[0]
+                    print(f"All {len(multi_agent_names)} LoRA adapters initialized before FSDP wrapping")
+                    print(f"Active adapter: {multi_agent_names[0]}")
+
         torch.distributed.barrier()
 
         if self.rank == 0:
@@ -669,6 +691,42 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
                 checkpoint_config=checkpoint_contents,
             )
 
+    @register(dispatch_mode=Dispatch.ONE_TO_ALL)
+    def set_active_lora(self, agent_role: str, lora_config: dict):
+        """
+        Switch the active LoRA adapter for training.
+        Only the active adapter receives gradient updates.
+
+        Args:
+            agent_role: Name of the agent role (e.g., "generator_initial")
+            lora_config: LoRA configuration dict with keys:
+                - lora_path: Optional path to pretrained adapter weights
+                - lora_rank: LoRA rank (r parameter)
+                - lora_alpha: LoRA alpha scaling factor
+                - target_modules: List of modules to apply LoRA
+                - lora_int_id: Integer ID for vLLM routing
+        """
+        multi_agent_names = self.config.get("agent_names", None)
+        assert self._is_lora and multi_agent_names, "set_active_lora requires multi-agent LoRA setup"
+        assert self._is_actor, "set_active_lora should only be called on actor worker"
+
+        print(f"Switching active LoRA adapter to: {agent_role}")
+
+        # Verify adapter exists (all adapters should be loaded during initialization)
+        if agent_role not in multi_agent_names:
+            raise ValueError(f"Adapter '{agent_role}' not found. Available adapters: {multi_agent_names}")
+
+        # Switch active adapter in PEFT model
+        self.actor_module_fsdp.set_adapter(agent_role)
+        for name, param in self.actor_module_fsdp.named_parameters():
+            if agent_role in name:
+                param.requires_grad = True
+            else:
+                param.requires_grad = False
+        print("Required grads updated for LoRA adapters:", [(name, param.shape) for name, param in self.actor_module_fsdp.named_parameters() if param.requires_grad == True])
+        self.current_active_lora = agent_role
+        print(f"Active adapter switched to: {agent_role}")
+
     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)
     @DistProfiler.annotate(color="red", role="actor_update")
     def update_actor(self, data: DataProto):
@@ -884,6 +942,62 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
         if self._is_offload_param:
             offload_fsdp_model_to_cpu(self.actor_module_fsdp)
 
+    @register(dispatch_mode=Dispatch.ONE_TO_ALL)
+    def save_single_lora_adapter(self, agent_name: str, save_path: str, global_step: int = 0):
+        """
+        Save a single LoRA adapter for a specific agent.
+        Used in multi-agent training to save each adapter separately.
+
+        Args:
+            agent_name: Name of the agent whose adapter to save
+            save_path: Path to save the adapter
+            global_step: Current training step (for logging)
+        """
+        from verl.utils.logger import log_with_rank
+
+        assert self._is_actor, "save_single_lora_adapter should only be called on actor"
+        assert self._is_lora, "save_single_lora_adapter requires LoRA to be enabled"
+        
+        # Switch to the target adapter
+        self.set_active_lora(agent_name, {})
+
+        if self._is_offload_param:
+            load_fsdp_model_to_gpu(self.actor_module_fsdp)
+
+        peft_model = getattr(self, "actor_module", self.actor_module_fsdp)
+        peft_config = {}
+
+        if dist.get_rank() == 0:
+            os.makedirs(save_path, exist_ok=True)
+            peft_config = asdict(peft_model.peft_config.get(agent_name, {}))
+            peft_config["task_type"] = peft_config["task_type"].value
+            peft_config["peft_type"] = peft_config["peft_type"].value
+            peft_config["target_modules"] = list(peft_config["target_modules"])
+        try:
+            if fsdp_version(self.actor_module_fsdp) > 0:
+                self.actor_module_fsdp = self.actor_module_fsdp.to(get_device_name())
+                lora_params = layered_summon_lora_params(self.actor_module_fsdp, adapter_name=agent_name)
+                # breakpoint()
+                if dist.get_rank() == 0:
+                    save_file(lora_params, os.path.join(save_path, "adapter_model.safetensors"))
+                    with open(os.path.join(save_path, "adapter_config.json"), "w", encoding="utf-8") as f:
+                        json.dump(peft_config, f, ensure_ascii=False, indent=4)
+        except Exception as e:
+            log_with_rank(
+                f"Save LoRA Adapter Error for {agent_name} ({e})", rank=dist.get_rank(), logger=logger, log_only_rank_0=True
+            )
+
+        dist.barrier()
+        log_with_rank(
+            f"[rank-{self.rank}]: Saved LoRA adapter '{agent_name}' to: {save_path}",
+            rank=dist.get_rank(),
+            logger=logger,
+            log_only_rank_0=True,
+        )
+
+        if self._is_offload_param:
+            offload_fsdp_model_to_cpu(self.actor_module_fsdp)
+
     @register(dispatch_mode=Dispatch.ONE_TO_ALL)
     def load_checkpoint(self, local_path, hdfs_path=None, del_local_after_load=False):
         assert self._is_actor or (not self._is_actor and self._is_rollout), (
diff --git a/verl/workers/rollout/vllm_rollout/vllm_async_server.py b/verl/workers/rollout/vllm_rollout/vllm_async_server.py
index 988dac40..edda7d24 100644
--- a/verl/workers/rollout/vllm_rollout/vllm_async_server.py
+++ b/verl/workers/rollout/vllm_rollout/vllm_async_server.py
@@ -25,6 +25,7 @@ from vllm import SamplingParams
 from vllm.engine.arg_utils import AsyncEngineArgs
 from vllm.entrypoints.logger import RequestLogger
 from vllm.entrypoints.openai.protocol import ChatCompletionRequest, ChatCompletionResponse, ErrorResponse
+from vllm.lora.request import LoRARequest
 from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
 from vllm.entrypoints.openai.serving_models import BaseModelPath, OpenAIServingModels
 from vllm.inputs import TokensPrompt
@@ -260,6 +261,9 @@ class AsyncvLLMServer(AsyncServerBase):
             enable_prefix_caching=True,
             trust_remote_code=trust_remote_code,
             seed=config.get("seed", 0),
+            enable_lora=True if self.config.model.get("lora_rank", 0) > 0 else False,
+            max_loras=len(self.config.get("agent_names", [])),
+            max_lora_rank=self.config.model.get("lora_rank", 0)
         )
 
         # init async llm engine
@@ -313,11 +317,14 @@ class AsyncvLLMServer(AsyncServerBase):
             assert isinstance(generator, ChatCompletionResponse)
             return JSONResponse(content=generator.model_dump())
 
-    async def generate(self, prompt_ids: list[int], sampling_params: dict[str, Any], request_id: str) -> list[int]:
+    async def generate(self, prompt_ids: list[int], sampling_params: dict[str, Any], request_id: str, lora_request: LoRARequest) -> list[int]:
         max_tokens = self.max_model_len - len(prompt_ids)
         sampling_params = SamplingParams(max_tokens=max_tokens, **sampling_params)
         prompt = TokensPrompt(prompt_token_ids=prompt_ids)
-        generator = self.engine.generate(prompt=prompt, sampling_params=sampling_params, request_id=request_id)
+        generator = self.engine.generate(
+            prompt=prompt, sampling_params=sampling_params, request_id=request_id,
+            lora_request=lora_request,
+        )
 
         # Get final response
         final_res: Optional[RequestOutput] = None
diff --git a/verl/workers/sharding_manager/fsdp_vllm.py b/verl/workers/sharding_manager/fsdp_vllm.py
index 1a9677df..92dd0dd8 100644
--- a/verl/workers/sharding_manager/fsdp_vllm.py
+++ b/verl/workers/sharding_manager/fsdp_vllm.py
@@ -200,7 +200,7 @@ class FSDPVLLMShardingManager(BaseShardingManager):
             peft_config = None
             peft_model = getattr(self.module, "_fsdp_wrapped_module", self.module)
             if hasattr(peft_model, "peft_config"):
-                peft_config = peft_model.peft_config.get("default", None)
+                peft_config = list(peft_model.peft_config.values())[0]
                 params = __collect_lora_params()
             else:
                 params = self.module.state_dict()
@@ -291,7 +291,7 @@ class FSDPVLLMShardingManager(BaseShardingManager):
                     peft_config=asdict(peft_config),
                     lora_tensors=updated_params,
                 )
-                self.inference_engine.llm_engine.add_lora(lora_reqest)
+                self.inference_engine.add_lora(lora_reqest)
                 logger.info(f"vLLM load weights, loaded_params: {len(updated_params)}")
                 return
             else:
