diff --git a/verl/experimental/agent_loop/agent_loop.py b/verl/experimental/agent_loop/agent_loop.py
index 274259cc..3cd08b64 100644
--- a/verl/experimental/agent_loop/agent_loop.py
+++ b/verl/experimental/agent_loop/agent_loop.py
@@ -94,6 +94,7 @@ class AsyncLLMServerManager:
         prompt_ids: list[int],
         sampling_params: dict[str, Any],
         image_data: Optional[list[Any]] = None,
+        lora_int_id: Optional[int] = None,
     ) -> TokenOutput:
         """Generate tokens from prompt ids.
 
@@ -111,6 +112,7 @@ class AsyncLLMServerManager:
             prompt_ids=prompt_ids,
             sampling_params=sampling_params,
             image_data=image_data,
+            lora_int_id=lora_int_id,
         )
         return output
 
@@ -816,3 +818,17 @@ class AgentLoopManager:
             await asyncio.gather(*tasks)
 
         asyncio.run(run_all())
+
+    def load_lora_adapter(self, lora_name: str, lora_path: str):
+        """Load LoRA adapter to all rollout server instances.
+
+        Args:
+            lora_name (str): LoRA adapter name.
+            lora_path (str): LoRA adapter path.
+        """
+        ray.get(
+            [
+                replica.load_lora_adapter.remote(lora_name=lora_name, lora_path=lora_path)
+                for replica in self.rollout_replicas
+            ]
+        )
diff --git a/verl/utils/dataset/rl_dataset.py b/verl/utils/dataset/rl_dataset.py
index 870c4751..788b5dbe 100644
--- a/verl/utils/dataset/rl_dataset.py
+++ b/verl/utils/dataset/rl_dataset.py
@@ -28,6 +28,7 @@ import torch
 from omegaconf import DictConfig, ListConfig
 from torch.utils.data import Dataset
 from transformers import PreTrainedTokenizer, ProcessorMixin
+import polars as pl
 
 import verl.utils.torch_functional as verl_F
 from verl.utils.model import compute_position_id_with_mask
@@ -153,9 +154,10 @@ class RLHFDataset(Dataset):
         dataframes = []
         for parquet_file in self.data_files:
             # read parquet files and cache
-            dataframe = datasets.load_dataset("parquet", data_files=parquet_file)["train"]
+            dataframe = pl.read_parquet(parquet_file)
             dataframes.append(dataframe)
-        self.dataframe: datasets.Dataset = datasets.concatenate_datasets(dataframes)
+        # Convert Polars DataFrame to HuggingFace Dataset via Arrow (zero-copy)
+        self.dataframe: datasets.Dataset = datasets.Dataset(pl.concat(dataframes).to_arrow())
 
         total = len(self.dataframe)
         print(f"dataset len: {len(self.dataframe)}")
@@ -441,6 +443,13 @@ class RLHFDataset(Dataset):
         # add index for each prompt
         if "extra_info" not in row_dict or row_dict["extra_info"] is None:
             row_dict["extra_info"] = dict()
+        # Handle extra_info as JSON string (for multi-agent support)
+        elif isinstance(row_dict["extra_info"], str):
+            import json
+            try:
+                row_dict["extra_info"] = json.loads(row_dict["extra_info"])
+            except json.JSONDecodeError:
+                row_dict["extra_info"] = dict()
         index = row_dict.get("extra_info", {}).get("index", 0)
         tools_kwargs = row_dict.get("extra_info", {}).get("tools_kwargs", {})
         interaction_kwargs = row_dict.get("extra_info", {}).get("interaction_kwargs", {})
diff --git a/verl/utils/fsdp_utils.py b/verl/utils/fsdp_utils.py
index 9b0d48fb..2464e34d 100644
--- a/verl/utils/fsdp_utils.py
+++ b/verl/utils/fsdp_utils.py
@@ -566,7 +566,7 @@ def fsdp2_clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinit
     return total_norm
 
 
-def layered_summon_lora_params(fsdp_module) -> OrderedDict:
+def layered_summon_lora_params(fsdp_module, adapter_name: str = "default") -> OrderedDict:
     from peft.utils.save_and_load import get_peft_model_state_dict
 
     def __prefix_submodules(module, prefix):
@@ -595,7 +595,9 @@ def layered_summon_lora_params(fsdp_module) -> OrderedDict:
                 continue
             if fsdp_version(submodule) > 0:
                 with FSDP.summon_full_params(submodule, writeback=False):
-                    sub_lora_params = get_peft_model_state_dict(peft_model, state_dict=submodule.state_dict())
+                    sub_lora_params = get_peft_model_state_dict(
+                        peft_model, state_dict=submodule.state_dict(), adapter_name=adapter_name
+                    )
                     sub_lora_params = {
                         f"{prefix}.{name}": param.full_tensor().detach().cpu()
                         if hasattr(param, "full_tensor")
@@ -608,10 +610,18 @@ def layered_summon_lora_params(fsdp_module) -> OrderedDict:
     return lora_params
 
 
-def collect_lora_params(module: FSDP, layered_summon: bool, base_sync_done: bool) -> OrderedDict:
+def collect_lora_params(
+    module: FSDP, layered_summon: bool, base_sync_done: bool, adapter_name: str = "default"
+) -> OrderedDict:
     """
     collect lora params or full params if base model is not ready in vllm
     work with if isinstance(self.module._fsdp_wrapped_module, PeftModel)
+
+    Args:
+        module: The FSDP wrapped module
+        layered_summon: Whether to use layered summon for memory efficiency
+        base_sync_done: Whether base model weights have been synced to vLLM
+        adapter_name: Name of the LoRA adapter to extract (for multi-agent support)
     """
     from peft.utils.save_and_load import get_peft_model_state_dict
 
@@ -624,11 +634,11 @@ def collect_lora_params(module: FSDP, layered_summon: bool, base_sync_done: bool
                     "To use layered_summon, you must make sure base-model is preloaded in vllm, e.g. let "
                     "rollout.load_format=safetensors"
                 )
-            lora_params = layered_summon_lora_params(module)
+            lora_params = layered_summon_lora_params(module, adapter_name=adapter_name)
         else:
             with FSDP.summon_full_params(module, writeback=False):
                 if base_sync_done:
-                    lora_params = get_peft_model_state_dict(peft_model)
+                    lora_params = get_peft_model_state_dict(peft_model, adapter_name=adapter_name)
                     lora_params = {
                         name: param.full_tensor().detach().cpu()
                         if hasattr(param, "full_tensor")
@@ -652,7 +662,7 @@ def collect_lora_params(module: FSDP, layered_summon: bool, base_sync_done: bool
             get_torch_device().empty_cache()
     else:
         if base_sync_done:
-            lora_params = get_peft_model_state_dict(peft_model)
+            lora_params = get_peft_model_state_dict(peft_model, adapter_name=adapter_name)
         else:
             model = peft_model.base_model.model
             orig_dev = "cpu" if "cpu" in str(next(model.parameters()).device) else get_device_name()
diff --git a/verl/utils/vllm/utils.py b/verl/utils/vllm/utils.py
index acf24398..4e4d2687 100644
--- a/verl/utils/vllm/utils.py
+++ b/verl/utils/vllm/utils.py
@@ -74,7 +74,6 @@ class VLLMHijack:
                 hf_to_vllm_mapper = None
                 if hasattr(model, "hf_to_vllm_mapper") and model.hf_to_vllm_mapper is not None:
                     hf_to_vllm_mapper = model.hf_to_vllm_mapper
-
                 if isinstance(lora_request, TensorLoRARequest):
                     lora = self._lora_model_cls.from_lora_tensors(
                         lora_model_id=lora_request.lora_int_id,
diff --git a/verl/workers/fsdp_workers.py b/verl/workers/fsdp_workers.py
index 24be3893..a804fd93 100644
--- a/verl/workers/fsdp_workers.py
+++ b/verl/workers/fsdp_workers.py
@@ -36,6 +36,8 @@ from torch.distributed.device_mesh import init_device_mesh
 from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
 from torch.distributed.fsdp.api import FullStateDictConfig, ShardedStateDictConfig, StateDictType
 
+from verl.workers.rollout.vllm_rollout.utils import VLLM_LORA_INT_ID
+
 try:
     # for torch 2.5+
     from torch.distributed.tensor import DTensor
@@ -438,6 +440,30 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
                 }
                 actor_module = get_peft_model(actor_module, LoraConfig(**lora_config))
 
+                # Check if multi-agent mode is enabled
+                multi_agent_names = self.config.get("agent_names", [])
+                if len(multi_agent_names) > 1 and (not self.config.get("share_policy")):
+                    # Multi-agent: Initialize all LoRA adapters before FSDP wrapping
+                    if self.rank == 0:
+                        print(f"[Multi-Agent LoRA] Initializing {len(multi_agent_names)} adapters: {multi_agent_names}")
+
+                    for adapter_name in multi_agent_names:
+                        if adapter_name != "default":
+                            actor_module.add_adapter(adapter_name=adapter_name, peft_config=LoraConfig(**lora_config))
+                            if self.rank == 0:
+                                print(f"[Multi-Agent LoRA] Added adapter: {adapter_name}")
+
+                    # FSDP auto wrapping requires all parameters to have requires_grad set correctly
+                    for name, param in actor_module.named_parameters():
+                        if "lora" in name and "default" not in name:
+                            param.requires_grad = True
+                        else:
+                            param.requires_grad = False
+
+                    self.current_active_lora = multi_agent_names[0]
+                    if self.rank == 0:
+                        print(f"[Multi-Agent LoRA] All adapters initialized. Active adapter: {multi_agent_names[0]}")
+
         self.use_orig_params = fsdp_config.get("use_orig_params", False)
         if self.config.actor.get("freeze_vision_tower", False):
             vision_tower = get_vl_model_vision_tower(actor_module)
@@ -657,8 +683,31 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
             load_fsdp_model_to_gpu(self.actor_module_fsdp)
         log_gpu_memory_usage("After load_fsdp_model_to_gpu", logger=logger)
 
-        peft_config = None
         peft_model = getattr(self.actor_module_fsdp, "_fsdp_wrapped_module", self.actor_module_fsdp)
+
+        # Check for multi-agent LoRA mode
+        agent_names = self.config.get("agent_names", [])
+        is_multi_agent = (
+            hasattr(peft_model, "peft_config")
+            and len(agent_names) > 1
+            and not self.config.get("share_policy", False)
+        )
+
+        if is_multi_agent:
+            # Multi-agent LoRA: sync ALL adapters with unique lora_int_ids
+            await self._sync_multi_agent_lora_weights(peft_model, agent_names)
+        else:
+            # Single-agent mode: original logic (backward compatible)
+            await self._sync_single_agent_weights(peft_model)
+
+        self.base_sync_done = True
+        # important: need to manually set the random states of each tp to be identical.
+        self.torch_random_states = get_torch_device().get_rng_state()
+        get_torch_device().set_rng_state(self.gen_random_states)
+
+    async def _sync_single_agent_weights(self, peft_model):
+        """Original single-agent weight sync logic (backward compatible)."""
+        peft_config = None
         if hasattr(peft_model, "peft_config"):  # LoRA
             peft_config = peft_model.peft_config.get("default", None)
             params = collect_lora_params(
@@ -726,10 +775,173 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
             await self.rollout.resume(tags=["kv_cache"])
         log_gpu_memory_usage("After resume kv_cache", logger=logger)
 
-        self.base_sync_done = True
-        # important: need to manually set the random states of each tp to be identical.
-        self.torch_random_states = get_torch_device().get_rng_state()
-        get_torch_device().set_rng_state(self.gen_random_states)
+    def _compute_weight_hash(self, params: dict, name: str = "") -> dict:
+        """Compute MD5 hash and statistics for weight verification.
+
+        Args:
+            params: Dictionary of parameter name -> tensor
+            name: Name for logging purposes
+
+        Returns:
+            Dictionary with MD5 hash and basic statistics
+        """
+        import hashlib
+
+        if not params:
+            return {"name": name, "num_params": 0, "md5": "empty", "num_elements": 0}
+
+        md5_hash = hashlib.md5()
+        total_count = 0
+
+        # Sort keys for deterministic hash computation
+        for param_name in sorted(params.keys()):
+            param = params[param_name]
+            if isinstance(param, DTensor):
+                param = param.full_tensor()
+            # Convert to bytes in a deterministic way (use contiguous float32)
+            param_bytes = param.float().contiguous().cpu().numpy().tobytes()
+            md5_hash.update(param_bytes)
+            total_count += param.numel()
+
+        return {
+            "name": name,
+            "num_params": len(params),
+            "num_elements": total_count,
+            "md5": md5_hash.hexdigest(),
+        }
+
+    async def _sync_multi_agent_lora_weights(self, peft_model, agent_names: list):
+        """Sync LoRA weights for all agents in multi-agent mode.
+
+        Each agent's adapter is synced with a unique lora_int_id (index + 1),
+        matching the mapping used in verl_engine.py for inference routing.
+
+        Args:
+            peft_model: The PEFT model containing all agent adapters
+            agent_names: List of agent names (e.g., ["agent_0", "agent_1", "agent_2"])
+        """
+        if self.rank == 0:
+            logger.info(f"[Multi-Agent LoRA] Syncing {len(agent_names)} adapters: {agent_names}")
+            logger.info(f"[Multi-Agent LoRA] Available adapters in peft_config: {list(peft_model.peft_config.keys())}")
+            logger.info(f"[Multi-Agent LoRA] base_sync_done={self.base_sync_done}")
+
+        # Validate agent_names match available adapters
+        available_adapters = set(peft_model.peft_config.keys())
+        for name in agent_names:
+            if name not in available_adapters:
+                raise ValueError(
+                    f"[Multi-Agent LoRA] Adapter '{name}' not found in peft_config. "
+                    f"Available adapters: {available_adapters}"
+                )
+
+        # Get first adapter's peft_config (they should all have the same structure)
+        first_adapter_name = agent_names[0]
+        peft_config = peft_model.peft_config[first_adapter_name]
+
+        # Step 1: Collect ALL adapter weights FIRST while model is on GPU
+        # (Same pattern as single-agent: collect once, then sync)
+        all_adapter_params = {}
+        layered_summon = self.config.rollout.get("layered_summon", False)
+
+        for adapter_name in agent_names:
+            adapter_params = collect_lora_params(
+                module=self.actor_module_fsdp,
+                layered_summon=layered_summon,
+                base_sync_done=True,  # Only sync LoRA params, not base model
+                adapter_name=adapter_name,
+            )
+            adapter_params = convert_weight_keys(
+                adapter_params, getattr(self.actor_module_fsdp, "_fsdp_wrapped_module", self.actor_module_fsdp)
+            )
+            all_adapter_params[adapter_name] = adapter_params
+
+            if self.rank == 0:
+                # Debug: log param shapes to verify correct collection
+                sample_key = next(iter(adapter_params.keys())) if adapter_params else None
+                sample_shape = adapter_params[sample_key].shape if sample_key else None
+                logger.info(
+                    f"[Multi-Agent LoRA] Collected adapter '{adapter_name}': "
+                    f"{len(adapter_params)} params, sample: {sample_key}={sample_shape}"
+                )
+                # Compute and log MD5 hash for weight verification
+                hash_info = self._compute_weight_hash(adapter_params, adapter_name)
+                logger.info(
+                    f"[Multi-Agent LoRA] Weight MD5 for '{adapter_name}': "
+                    f"md5={hash_info['md5']}, "
+                    f"num_params={hash_info['num_params']}, "
+                    f"num_elements={hash_info['num_elements']}"
+                )
+
+        # Step 2: Handle base model sync if needed
+        # - First time sync (base_sync_done=False), OR
+        # - sleep_level=2: base model weights are destroyed during each sleep cycle
+        sleep_level = getattr(self.rollout, "sleep_level", None)
+        need_base_sync = not self.base_sync_done or sleep_level == 2
+        if self.rank == 0 and need_base_sync:
+            reason = "first sync" if not self.base_sync_done else f"sleep_level={sleep_level}"
+            logger.info(f"[Multi-Agent LoRA] Base model sync needed: {reason}")
+        if need_base_sync:
+            base_model_params = collect_lora_params(
+                module=self.actor_module_fsdp,
+                layered_summon=layered_summon,
+                base_sync_done=False,
+                adapter_name=first_adapter_name,
+            )
+            base_model_params = {replace_lora_wrapper(k, peft_config): v for k, v in base_model_params.items()}
+            base_model_params = convert_weight_keys(
+                base_model_params, getattr(self.actor_module_fsdp, "_fsdp_wrapped_module", self.actor_module_fsdp)
+            )
+
+        # Step 3: Offload model to CPU (same as single-agent)
+        log_gpu_memory_usage("Before offload_fsdp_model_to_cpu (multi-agent)", logger=logger)
+        if self._is_offload_param:
+            offload_fsdp_model_to_cpu(self.actor_module_fsdp)
+        log_gpu_memory_usage("After offload_fsdp_model_to_cpu (multi-agent)", logger=logger)
+
+        set_expandable_segments(False)
+
+        if self.config.rollout.free_cache_engine:
+            await self.rollout.resume(tags=["weights"])
+        log_gpu_memory_usage("After resume weights (multi-agent)", logger=logger)
+
+        # Step 4: Sync base model if needed (first sync or sleep_level=2)
+        if need_base_sync:
+            device = get_device_id()
+            per_tensor_base_params = (
+                (name, param.to(device, non_blocking=True).full_tensor() if isinstance(param, DTensor) else param)
+                for name, param in base_model_params.items()
+            )
+            await self.rollout.update_weights(per_tensor_base_params, base_sync_done=False)
+            del base_model_params, per_tensor_base_params
+            aggressive_empty_cache(force_sync=False)
+
+        # Step 5: Sync each agent's LoRA adapter with its unique lora_int_id
+        for idx, adapter_name in enumerate(agent_names):
+            lora_int_id = idx + VLLM_LORA_INT_ID  # Consistent mapping: agent_names.index(name) + VLLM_LORA_INT_ID
+            adapter_params = all_adapter_params[adapter_name]
+            adapter_peft_config = peft_model.peft_config[adapter_name]
+
+            per_tensor_params = adapter_params.items() if isinstance(adapter_params, dict) else adapter_params
+            await self.rollout.update_weights(
+                per_tensor_params,
+                peft_config=adapter_peft_config,
+                base_sync_done=True,
+                lora_int_id=lora_int_id,
+            )
+
+            if self.rank == 0:
+                logger.info(f"[Multi-Agent LoRA] Synced adapter '{adapter_name}' with lora_int_id={lora_int_id}")
+
+            del adapter_params
+            aggressive_empty_cache(force_sync=False)
+
+        del all_adapter_params
+        log_gpu_memory_usage("After update_weights (multi-agent)", logger=logger)
+        aggressive_empty_cache(force_sync=True)
+
+        if self.config.rollout.free_cache_engine:
+            await self.rollout.resume(tags=["kv_cache"])
+        log_gpu_memory_usage("After resume kv_cache (multi-agent)", logger=logger)
 
     async def trainer_mode(self):
         """Context switch hybridengine to trainer mode."""
@@ -749,6 +961,110 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
         self.gen_random_states = get_torch_device().get_rng_state()
         get_torch_device().set_rng_state(self.torch_random_states)
 
+    @register(dispatch_mode=Dispatch.ONE_TO_ALL)
+    def set_active_lora(self, agent_role: str, lora_config: dict = None):
+        """
+        Switch the active LoRA adapter for training.
+        Only the active adapter receives gradient updates.
+
+        Args:
+            agent_role: Name of the agent role (e.g., "generator_initial")
+            lora_config: LoRA configuration dict (unused, for API compatibility)
+        """
+        multi_agent_names = self.config.get("agent_names", None)
+        if not self._is_lora or not multi_agent_names or len(multi_agent_names) <= 1:
+            return  # Not multi-agent LoRA mode
+
+        if not self._is_actor:
+            return  # Only actor worker needs to switch adapters
+
+        if self.rank == 0:
+            print(f"[Multi-Agent LoRA] Switching active adapter to: {agent_role}")
+
+        # Verify adapter exists (all adapters should be loaded during initialization)
+        if agent_role not in multi_agent_names:
+            raise ValueError(f"Adapter '{agent_role}' not found. Available adapters: {multi_agent_names}")
+
+        # Switch active adapter in PEFT model
+        peft_model = getattr(self.actor_module_fsdp, "_fsdp_wrapped_module", self.actor_module_fsdp)
+        peft_model.set_adapter(agent_role)
+
+        # Update requires_grad for the new active adapter
+        for name, param in self.actor_module_fsdp.named_parameters():
+            if agent_role in name:
+                param.requires_grad = True
+            else:
+                param.requires_grad = False
+
+        self.current_active_lora = agent_role
+        if self.rank == 0:
+            trainable_params = [(name, param.shape) for name, param in self.actor_module_fsdp.named_parameters() if param.requires_grad]
+            print(f"[Multi-Agent LoRA] Active adapter: {agent_role}, trainable params: {len(trainable_params)}")
+
+    @register(dispatch_mode=Dispatch.ONE_TO_ALL)
+    def save_single_lora_adapter(self, agent_name: str, save_path: str, global_step: int = 0):
+        """
+        Save a single LoRA adapter for a specific agent.
+        Used in multi-agent training to save each adapter separately.
+
+        Args:
+            agent_name: Name of the agent whose adapter to save
+            save_path: Path to save the adapter
+            global_step: Current training step (for logging)
+        """
+        import json
+        import os
+
+        from safetensors.torch import save_file
+
+        from verl.utils.fsdp_utils import layered_summon_lora_params, load_fsdp_model_to_gpu, offload_fsdp_model_to_cpu
+        from verl.utils.logger import log_with_rank
+
+        if not self._is_actor or not self._is_lora:
+            return
+
+        # Switch to the target adapter
+        self.set_active_lora(agent_name, {})
+
+        if self._is_offload_param:
+            load_fsdp_model_to_gpu(self.actor_module_fsdp)
+
+        peft_model = getattr(self.actor_module_fsdp, "_fsdp_wrapped_module", self.actor_module_fsdp)
+        peft_config = {}
+
+        if dist.get_rank() == 0:
+            os.makedirs(save_path, exist_ok=True)
+            adapter_peft_config = peft_model.peft_config.get(agent_name, {})
+            if adapter_peft_config:
+                peft_config = asdict(adapter_peft_config)
+                peft_config["task_type"] = peft_config["task_type"].value
+                peft_config["peft_type"] = peft_config["peft_type"].value
+                peft_config["target_modules"] = list(peft_config["target_modules"])
+
+        try:
+            if fsdp_version(self.actor_module_fsdp) > 0:
+                self.actor_module_fsdp = self.actor_module_fsdp.to(get_device_name())
+                lora_params = layered_summon_lora_params(self.actor_module_fsdp, adapter_name=agent_name)
+                if dist.get_rank() == 0:
+                    save_file(lora_params, os.path.join(save_path, "adapter_model.safetensors"))
+                    with open(os.path.join(save_path, "adapter_config.json"), "w", encoding="utf-8") as f:
+                        json.dump(peft_config, f, ensure_ascii=False, indent=4)
+        except Exception as e:
+            log_with_rank(
+                f"Save LoRA Adapter Error for {agent_name} ({e})", rank=dist.get_rank(), logger=logger, log_only_rank_0=True
+            )
+
+        dist.barrier()
+        log_with_rank(
+            f"[rank-{self.rank}]: Saved LoRA adapter '{agent_name}' to: {save_path}",
+            rank=dist.get_rank(),
+            logger=logger,
+            log_only_rank_0=True,
+        )
+
+        if self._is_offload_param:
+            offload_fsdp_model_to_cpu(self.actor_module_fsdp)
+
     @register(dispatch_mode=Dispatch.ONE_TO_ALL)
     def init_model(self):
         from verl.workers.actor import DataParallelPPOActor
diff --git a/verl/workers/rollout/vllm_rollout/vllm_async_server.py b/verl/workers/rollout/vllm_rollout/vllm_async_server.py
index 287c8dc4..3515cc4d 100644
--- a/verl/workers/rollout/vllm_rollout/vllm_async_server.py
+++ b/verl/workers/rollout/vllm_rollout/vllm_async_server.py
@@ -269,13 +269,18 @@ class vLLMHttpServerBase:
 
         # update lora-related args
         if self.model_config.lora_rank > 0:
+            # Support multi-agent LoRA: set max_loras based on agent_names config
+            agent_names = self.config.get("agent_names", [])
+            max_loras = max(1, len(agent_names)) if agent_names else 1
             args.update(
                 {
                     "enable_lora": True,
-                    "max_loras": 1,
+                    "max_loras": max_loras,
                     "max_lora_rank": get_vllm_max_lora_rank(self.model_config.lora_rank),
                 }
             )
+            if self.replica_rank == 0 and max_loras > 1:
+                logger.info(f"[Multi-Agent LoRA] Configured vLLM with max_loras={max_loras} for agents: {agent_names}")
 
         server_args = ["serve", self.model_config.local_path]
         for k, v in args.items():
@@ -376,6 +381,7 @@ class vLLMHttpServerBase:
         sampling_params: dict[str, Any],
         request_id: str,
         image_data: Optional[list[Any]] = None,
+        lora_int_id: Optional[int] = None,
     ) -> TokenOutput:
         """Generate sequence with token-in-token-out."""
         # TODO(@wuxibin): switch to `/generate` http endpoint once multi-modal support ready.
@@ -391,11 +397,17 @@ class vLLMHttpServerBase:
         # Add lora request
         lora_request = None
         if self.model_config.lora_rank > 0:
+            if lora_int_id is None:
+                lora_int_id = VLLM_LORA_INT_ID
+                lora_name = VLLM_LORA_NAME
+            else:
+                lora_name = f"{lora_int_id}"
             # Make sure we also check that the lora is already loaded in the engine
-            lora_loaded = VLLM_LORA_INT_ID in await self.engine.list_loras()
+            list_loras = await self.engine.list_loras()
+            lora_loaded = lora_int_id in list_loras
             if lora_loaded:
                 lora_request = LoRARequest(
-                    lora_name=VLLM_LORA_NAME, lora_int_id=VLLM_LORA_INT_ID, lora_path=VLLM_LORA_PATH
+                    lora_name=lora_name, lora_int_id=lora_int_id, lora_path=VLLM_LORA_PATH
                 )
 
         generator = self.engine.generate(
diff --git a/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py b/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
index 63af4f79..356c2ad8 100644
--- a/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
+++ b/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py
@@ -525,11 +525,15 @@ class vLLMAsyncRollout(BaseRollout):
         self.tokenizer = self.model_config.tokenizer
         self.inference_engine: WorkerWrapperBase = None
         self.address = self._init_zeromq()
-        self.lora_config = (
-            {"max_loras": 1, "max_lora_rank": get_vllm_max_lora_rank(self.model_config.lora_rank)}
-            if self.model_config.lora_rank > 0
-            else {}
-        )
+        if self.model_config.lora_rank > 0:
+            # Support multi-agent LoRA: set max_loras based on agent_names config
+            # max_loras = getattr(self.model_config, "max_loras", 1)
+            self.lora_config = {
+                "max_loras": 3, # Warning: Hardcoded for now. TODO: make it configurable
+                "max_lora_rank": get_vllm_max_lora_rank(self.model_config.lora_rank),
+            }
+        else:
+            self.lora_config = {}
 
         if config.layered_summon or (config.expert_parallel_size > 1 and not _check_vllm_version_for_sleep_level()):
             logger.warning("Setting the sleep level to 1 may cause a memory overflow.")
@@ -624,25 +628,36 @@ class vLLMAsyncRollout(BaseRollout):
         if self.config.free_cache_engine:
             self.inference_engine.sleep(level=self.sleep_level)
 
-    async def update_weights(self, weights: Generator[tuple[str, torch.Tensor], None, None], **kwargs):
+    async def update_weights(
+        self,
+        weights: Generator[tuple[str, torch.Tensor], None, None],
+        lora_int_id: int = None,
+        **kwargs,
+    ):
         """Update the weights of the rollout model.
 
         Args:
             weights: A generator that yields the name of the weight tensor and the tensor itself.
+            lora_int_id: The integer ID for this LoRA adapter (for multi-agent support).
+                         If None, uses VLLM_LORA_INT_ID for backward compatibility.
         """
         peft_config, base_sync_done = kwargs.get("peft_config", None), kwargs.get("base_sync_done", False)
         if peft_config and base_sync_done:
+            # Determine the lora_int_id to use (multi-agent support)
+            effective_lora_int_id = lora_int_id if lora_int_id is not None else VLLM_LORA_INT_ID
+            effective_lora_name = f"{effective_lora_int_id}"
+
             # In async mode, make sure the old lora is removed before adding the new one
-            self.inference_engine.worker.remove_lora(VLLM_LORA_INT_ID)
+            self.inference_engine.worker.remove_lora(effective_lora_int_id)
             lora_request = TensorLoRARequest(
-                lora_name=VLLM_LORA_NAME,
-                lora_int_id=VLLM_LORA_INT_ID,
+                lora_name=effective_lora_name,
+                lora_int_id=effective_lora_int_id,
                 lora_path=VLLM_LORA_PATH,
                 peft_config=asdict(peft_config),
                 lora_tensors=dict(weights),
             )
             self.inference_engine.worker.add_lora(lora_request)
-            logger.info(f"vLLM load weights, loaded_params: {len(weights)}")
+            logger.info(f"vLLM load weights for lora_int_id={effective_lora_int_id}, loaded_params: {len(weights)}")
         else:
             from verl.utils.vllm.patch import patch_vllm_moe_model_weight_loader
 
diff --git a/verl/workers/sharding_manager/fsdp_vllm.py b/verl/workers/sharding_manager/fsdp_vllm.py
index fb6d7469..87f9a94c 100644
--- a/verl/workers/sharding_manager/fsdp_vllm.py
+++ b/verl/workers/sharding_manager/fsdp_vllm.py
@@ -202,7 +202,8 @@ class FSDPVLLMShardingManager(BaseShardingManager):
             peft_config = None
             peft_model = getattr(self.module, "_fsdp_wrapped_module", self.module)
             if hasattr(peft_model, "peft_config"):
-                peft_config = peft_model.peft_config.get("default", None)
+                # Support multi-agent LoRA: get first available adapter config instead of hardcoded "default"
+                peft_config = list(peft_model.peft_config.values())[0] if peft_model.peft_config else None
                 params = __collect_lora_params()
             else:
                 params = self.module.state_dict()
