hydra:
  searchpath:
    - pkg://rllm.trainer.config
    - pkg://verl.trainer.config

defaults:
  - ppo_trainer
  - _self_

# RLLM-specific base settings (from agent_ppo_trainer)
actor_rollout_ref:
  rollout:
    mode: async
    agent:
      num_workers: 0
    val_kwargs:
      do_sample: True

data:
  gen_batch_size: ${mul:${data.train_batch_size},${rllm.rejection_sample.multiplier}}

rllm:
  agent:
    name: math_agent
    max_steps: 20
    trajectory_timeout: null
    overlong_filter: False
    agent_args: {}
    engine_args: {}
  env:
    name: custom
    env_args: {}
  disable_thinking: False
  accumulate_reasoning: False
  mask_truncated_samples: False
  rejection_sample:
    enable: False
    multiplier: 1

  # Multi-agent workflow overrides
  workflow:
    use_workflow: True
    name: single_turn_workflow
    workflow_args:
      agent_cls: null
      agent_args: {}
      env_cls: null
      env_args: {}
      timeout: 1e6
      gamma: 0.0 # no discounting
      reward_bonus_coeff: 0.0 # no reward shaping
      max_refinement_iterations: 3
      reward_function: null
    n_parallel_tasks: 256
    retry_limit: 3

  # Enable stepwise advantage for multi-agent trajectories
  stepwise_advantage:
    enable: True
    mode: per_step
    normalize_by_steps: False

  # Enable compact filtering to handle multi-agent edge cases
  compact_filtering:
    enable: False  # Can be enabled per example
    mask_max_prompt_length_exceeded: True
    mask_max_response_length_exceeded: True
    mask_env_done: False
    mask_max_turns_exceeded: False
    mask_timeout: True
    mask_unknown: False
    mask_error: True

fireworks:
  deployment_id: null
  model_id_prefix: test-model
  concurrency: 32

ray_init:
  include_dashboard: True

# Multi-agent training specific settings
trainer:
  log_episodes: false
  episode_log_dir: logs/${trainer.project_name}/${trainer.experiment_name}

  # Agent names for multi-agent systems
  # Override this in specific training scripts with actual agent names
  agent_names: null  # e.g., ['generator', 'evaluator', 'refiner']

  # Policy sharing configuration
  share_policy: False  # Each agent has separate policy by default

  # LoRA configuration for multi-agent
  lora_adapter_path: '/tmp/rllm_tmp_lora'
  ori_single_policy_no_lora_mode: False

  # Validation configuration
  val_before_train: False

  # Default logging for multi-agent
  project_name: multi-agent-workflow
  experiment_name: multi-agent-training

# Actor, rollout, and model configuration optimized for multi-agent
actor_rollout_ref:
  model:
    use_remove_padding: True

  actor:
    loss_agg_mode: seq-mean-token-mean
    use_dynamic_bsz: True
    ppo_max_token_len_per_gpu: 32768
    clip_ratio_high: 0.28
    fsdp_config:
      param_offload: True
      optimizer_offload: True

  rollout:
    # Higher sampling for multi-agent exploration
    n: 8
    temperature: 0.6
    tensor_model_parallel_size: 1
    enforce_eager: False
    gpu_memory_utilization: 0.8
    val_kwargs:
      n: 1
      temperature: 0.6
      top_p: 0.95

  ref:
    fsdp_config:
      param_offload: True

# Algorithm configuration for multi-agent
algorithm:
  adv_estimator: grpo
